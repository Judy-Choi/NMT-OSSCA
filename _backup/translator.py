#!/usr/bin/env python3
"""
ê¸°ìˆ  ë¬¸ì„œ ë²ˆì—­ ìŠ¤í¬ë¦½íŠ¸ (LangChain & OpenAI)

ì‚¬ìš©ë²•:
1. .env íŒŒì¼ì— OpenAI API í‚¤ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. (OPENAI_API_KEY=your_key)
2. pip install -r requirements.txt
3. python ./translator.py
"""
import json
import os
from pathlib import Path
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

def initialize_llm(api_key: str, model_name: str) -> ChatOpenAI:
    """LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    return ChatOpenAI(model=model_name, temperature=0.1, openai_api_key=api_key)

def prepare_prompt_template(prompt_path: str, glossary_path: str) -> str:
    """ë²ˆì—­ í”„ë¡¬í”„íŠ¸ì™€ ë‹¨ì–´ì‚¬ì „ì„ ê²°í•©í•˜ì—¬ ìµœì¢… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    with open(prompt_path, 'r', encoding='utf-8') as f:
        nmt_prompt_template = f.read()
    with open(glossary_path, 'r', encoding='utf-8') as f:
        glossary_data = json.load(f)
        
    instructions = []
    for entry in glossary_data:
        source = entry["source"]
        target = entry["target"]
        if isinstance(target, list):
            target_str = json.dumps(target, ensure_ascii=False)
            instructions.append(f'- "{source}" â†’ {target_str}')
        else:
            instructions.append(f'- "{source}" â†’ "{target}"')
    glossary_instructions = "\n".join(instructions)
    
    return nmt_prompt_template.replace("{glossary_instructions}", glossary_instructions)

def split_markdown_by_headings(markdown_content: str) -> list[str]:
    """ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ë¥¼ ì œëª©(#) ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    chunks = []
    current_chunk_lines = []
    for line in markdown_content.splitlines():
        if line.startswith('#'):
            if current_chunk_lines:
                chunks.append("\n".join(current_chunk_lines))
            current_chunk_lines = [line]
        else:
            current_chunk_lines.append(line)
    if current_chunk_lines:
        chunks.append("\n".join(current_chunk_lines))
    return chunks

def translate_document(llm: ChatOpenAI, source_content: str, prompt_template: str) -> str:
    """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê³ , ê° ì²­í¬ë¥¼ ë²ˆì—­í•˜ì—¬ ìµœì¢… ê²°ê³¼ë¬¼ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    chunks = split_markdown_by_headings(source_content)
    print(f"ğŸ”ª ë¬¸ì„œë¥¼ {len(chunks)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
    
    translated_chunks = []
    for i, chunk in enumerate(chunks):
        if not chunk.strip():
            translated_chunks.append(chunk)
            continue

        print(f"\nâ³ ì²­í¬ {i+1}/{len(chunks)} ë²ˆì—­ ì¤‘...")
        prompt = prompt_template.replace("{source}", chunk)
        final_chunk = llm.invoke(prompt).content
        print(f"   => âœ… ì™„ë£Œ")
        translated_chunks.append(final_chunk)
        
    return "\n".join(translated_chunks)

def save_result(content: str, output_path: str):
    """ìµœì¢… ë²ˆì—­ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)

def main():
    """ìŠ¤í¬ë¦½íŠ¸ì˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    load_dotenv(override=True)
    
    # --- ì„¤ì • ---
    source_path = "./source_docs/models.md"
    prompt_path = "./prompts/nmt.yaml"
    glossary_path = "./glossary/main.json"
    output_path = "./output/models_ko.md"
    model_name = "gpt-3.5-turbo"
    api_key = os.getenv("OPENAI_API_KEY")
    
    # --- ì´ˆê¸°í™” ---
    llm = initialize_llm(api_key, model_name)
    final_prompt_template = prepare_prompt_template(prompt_path, glossary_path)
    
    # --- ì‹¤í–‰ ---
    with open(source_path, 'r', encoding='utf-8') as f:
        source_content = f.read()

    translated_content = translate_document(llm, source_content, final_prompt_template)

    # --- ê²°ê³¼ ì €ì¥ ---
    save_result(translated_content, output_path)

    print(f"\nğŸ‰ ë²ˆì—­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"   - ê²°ê³¼: {output_path}")


if __name__ == "__main__":
    main() 