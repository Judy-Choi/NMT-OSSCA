#!/usr/bin/env python3
"""
Streamlit ê¸°ë°˜ì˜ ëŒ€í™”í˜• ë¬¸ì„œ ë²ˆì—­ê¸°
"""
import os
import time
from pathlib import Path
import streamlit as st
from dotenv import load_dotenv
from langchain_litellm import ChatLiteLLM
from tools.utils import (
    load_prompt_template,
    load_glossary,
    get_glossary_terms,
    prepare_final_prompt,
    split_markdown_by_headings,
    load_and_display_existing_translation
)
from tools.display import highlight_terms

# --- Streamlit UI ---

st.set_page_config(layout="wide", page_title="AI ë¬¸ì„œ ë²ˆì—­ê¸°", page_icon="ğŸ¤—")
st.title("ğŸ¤— AI ë¬¸ì„œ ë²ˆì—­ê¸°")

# --- Initialize session state ---
if "translation_done" not in st.session_state:
    st.session_state.translation_done = False
if "source_chunks" not in st.session_state:
    st.session_state.source_chunks = []
if "show_progress_view" not in st.session_state:
    st.session_state.show_progress_view = False
if "mtpe_exist" not in st.session_state:
    st.session_state.mtpe_exist = False
if "mt_exist" not in st.session_state:
    st.session_state.mt_exist = False

# --- Sidebar for Settings ---
with st.sidebar:
    st.header("âš™ï¸ ë²ˆì—­ ì„¤ì •")
    
    load_dotenv()

    # claude-opus-4-20250514, gpt-4o ë“± ì‚¬ìš© ê°€ëŠ¥
    # ì¶”í›„ gemini ì¶”ê°€ ì˜ˆì •
    model_name = st.text_input("ëª¨ë¸ ì´ë¦„", value="claude-opus-4-20250514")
    
    # Load appropriate API key based on model name
    if 'gpt' in model_name:
        api_key = os.getenv("OPENAI_API_KEY", "")
    elif 'claude' in model_name:
        api_key = os.getenv("ANTHROPIC_API_KEY", "")
    else:
        api_key = ""  # Default to empty if no matching model name

    api_key = st.text_input(
        "API Key", 
        value=api_key, 
        type="password",
        help="API í‚¤ê°€ ì—†ìœ¼ë©´ í•´ë‹¹ í”Œë«í¼ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”."
    )  

    st.subheader("íŒŒì¼ ê²½ë¡œ")
    source_path = st.text_input("ì›ë³¸ ë¬¸ì„œ", value="./source_docs/models.md")
    prompt_path = st.text_input("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿", value="./prompts/nmt.yaml")
    glossary_path = st.text_input("ë‹¨ì–´ì‚¬ì „", value="./glossary/glossary.json")
    mt_path = st.text_input("ë²ˆì—­ ê²°ê³¼ ì €ì¥ ê²½ë¡œ", value="./mt/models_ko.md")
    mtpe_path = st.text_input("ë²ˆì—­ ìˆ˜ì • ê²°ê³¼ ì €ì¥ ê²½ë¡œ", value="./mtpe/models_ko.md")

    if st.button("ë²ˆì—­ ì‹œì‘", type="primary"):
        st.session_state.show_progress_view = True
        st.session_state.translation_done = False
        st.rerun()

# --- Main App Logic ---
if st.session_state.show_progress_view:
    st.session_state.show_progress_view = False  # í•œë²ˆë§Œ ì‹¤í–‰ë˜ë„ë¡ ì¬ì„¤ì •

    if not api_key:
        st.error("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not all([source_path, prompt_path, glossary_path, mt_path]):
        st.error("ëª¨ë“  íŒŒì¼ ê²½ë¡œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        if Path(mtpe_path).exists():
            st.success(f"âœ… ê¸°ê³„ë²ˆì—­ ì‚¬í›„êµì • íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤: {mtpe_path}")
            st.session_state.mtpe_exist = True
            time.sleep(2)
            st.rerun()

        elif Path(mt_path).exists():
            st.success(f"âœ… ê¸°ê³„ë²ˆì—­ ê²°ê³¼ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤: {mt_path}")
            st.session_state.mt_exist = True
            st.session_state.mtpe_exist = False
            time.sleep(2)
            st.rerun()

        else:
            # Proceed with LLM translation as before
            try:
                # 1. ì´ˆê¸°í™”
                # llm = ChatOpenAI(model=model_name, temperature=0.1, openai_api_key=api_key)
                llm = ChatLiteLLM(
                    model=model_name,
                    temperature=0.1,
                    api_key=api_key
                )
                base_prompt = load_prompt_template(prompt_path)
                glossary_data = load_glossary(glossary_path)
                final_prompt_template = prepare_final_prompt(base_prompt, glossary_data)
                source_terms, target_terms = get_glossary_terms(glossary_data)

                # 2. ì›ë³¸ ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
                with open(source_path, 'r', encoding='utf-8') as f:
                    source_content = f.read()
                
                source_chunks = split_markdown_by_headings(source_content)
                st.session_state.source_chunks = source_chunks
                
                st.info(f"âœ… ì´ {len(source_chunks)}ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë²ˆì—­ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

                # 3. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë²ˆì—­ ë° ê²°ê³¼ í‘œì‹œ
                for i, chunk in enumerate(source_chunks):
                    st.subheader(f"ë¬¸ë‹¨ {i+1}/{len(source_chunks)}")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("### ì›ë³¸")
                        highlighted_source = highlight_terms(chunk, source_terms)
                        with st.container(border=True):
                            st.markdown(highlighted_source, unsafe_allow_html=True)

                    with col2:
                        st.markdown("### ë²ˆì—­ ê²°ê³¼")
                        with st.container(border=True):
                            if not chunk.strip():
                                final_chunk = chunk
                                st.markdown(final_chunk)
                            else:
                                prompt = final_prompt_template.replace("{source}", chunk)
                                
                                placeholder = st.empty()
                                final_chunk = ""
                                try:
                                    for response in llm.stream(prompt):
                                        content = response.content
                                        if content:
                                            final_chunk += content
                                            placeholder.markdown(final_chunk + "â–Œ") # ì‹¤ì‹œê°„ ì»¤ì„œ íš¨ê³¼
                                    
                                    highlighted_final = highlight_terms(final_chunk, target_terms)
                                    placeholder.markdown(highlighted_final, unsafe_allow_html=True)
                                except Exception as e:
                                    st.error(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                                    final_chunk = f"ì˜¤ë¥˜: {e}"

                            st.session_state[f"edited_chunk_{i}"] = final_chunk

                st.session_state.translation_done = True
                # st.success("ğŸ‰ ë²ˆì—­ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì ì‹œ í›„ ìˆ˜ì • ëª¨ë“œë¡œ ì „í™˜ë©ë‹ˆë‹¤.")

                # Save the completed translation result to mt_path
                final_chunks = []
                for i in range(len(source_chunks)):
                    edited_content = st.session_state.get(f"edited_chunk_{i}", "")
                    final_chunks.append(edited_content)
                
                final_content = "\n".join(final_chunks)
                
                output_dir = Path(mt_path).parent
                output_dir.mkdir(parents=True, exist_ok=True)
                with open(mt_path, 'w', encoding='utf-8') as f:
                    f.write(final_content)
                
                st.success(f"âœ… ë²ˆì—­ì´ ì™„ë£Œë˜ì–´ ë‹¤ìŒ íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {mt_path}")
                st.session_state.mt_exist = True

                time.sleep(2)
                st.rerun()

            except FileNotFoundError as e:
                st.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e.filename}")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if st.session_state.mtpe_exist:
    load_and_display_existing_translation(source_path, mtpe_path, glossary_path, "ê¸°ê³„ë²ˆì—­ ì‚¬í›„êµì • ê²°ê³¼", mtpe_path)

elif st.session_state.mt_exist:
    load_and_display_existing_translation(source_path, mt_path, glossary_path, "ê¸°ê³„ë²ˆì—­ ê²°ê³¼", mtpe_path) 